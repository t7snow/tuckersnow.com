---
title: "2025 Q1 Paper Reading List"
description: "my reading list - of papers"
---

# 2025 Q1 Paper Reading List


I wanted to compile a list of some papers I want to get around to this semester. I will have notes on these as well on this page, so stay on the look out if you are lazy. 

If you have any recommendations, for real, dm me on Twitter @t7snow !

### foundational 
ðŸ”µ *Computing Machinery and Intelligence* by Alan Turing

ðŸ”µ *A Mathematical Theory of Communication* by Claude Shannon

ðŸ”µ *Time, clocks, and the ordering of events in a distributed system* - Leslie Lamport

ðŸ”µ *Communicating Sequential Processes* - C. A. R. Hoare

ðŸ”µ *The Art of Compiler Design: Theory and Practice* - Alfred V. Aho

### machine learning
ðŸ”µ *Attention Is All You Need* - Vaswani et al. 

ðŸ”µ *Generative Adversarial Nets* - Ian Goodfellow et al.

ðŸ”µ  *Language Models are Few-Shot Learners* - by Tom B. Brown et al.

ðŸ”µ *Memorizing Transformers* - Yuhuai Wu et al.

ðŸ”µ *Gradient-Based Learning Applied to Document Recognition* - Yann LeCun et al.

### networking
ðŸ”µ *End-to-End Arguments in System Design* - J.H. Saltzer, D.P. Reed, and D.D. Clark

ðŸ”µ *Congestion avoidance and control* - V. Jacobson

ðŸ”µ *A scalable, commodity data center network architecture* - Mohammad Al-Fares et al.

ðŸ”µ *The Design Philosophy of the DARPA Internet Protocols* - David D. Clark

ðŸ”µ *Software-Defined Networking: A Comprehensive Survey* - Diego Kreutz et al.

ðŸ”µ *The Click Modular Router* - Eddie Kohler

### rf ? 
### Ilya 30u30 papers

ðŸ”µ [**The Annotated Transformer**](https://nlp.seas.harvard.edu/annotated-transformer/)

ðŸ”µ [**The First Law of Complexodynamics**](https://scottaaronson.blog/?p=762)

ðŸ”µ [**The Unreasonable Effectiveness of RNNs**](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)

ðŸ”µ [**Understanding LSTM Networks**](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)

ðŸ”µ [**Recurrent Neural Network Regularization**](https://arxiv.org/pdf/1409.2329.pdf)

ðŸ”µ [**Keeping Neural Networks Simple by Minimizing the Description Length of the Weights**](https://www.cs.toronto.edu/~hinton/absps/colt93.pdf)
  
ðŸ”µ [**Pointer Networks**](https://arxiv.org/pdf/1506.03134.pdf)

ðŸ”µ [**ImageNet Classification with Deep Convolutional Neural Networks**](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)

ðŸ”µ [**Order Matters: Sequence to sequence for sets**](https://arxiv.org/pdf/1511.06391.pdf)

ðŸ”µ [**GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism**](https://arxiv.org/pdf/1811.06965.pdf)

ðŸ”µ [**Deep Residual Learning for Image Recognition**](https://arxiv.org/pdf/1512.03385.pdf)

ðŸ”µ [**Multi-Scale Context Aggregation by Dilated Convolutions**](https://arxiv.org/pdf/1511.07122.pdf)

ðŸ”µ [**Neural Quantum Chemistry**](https://arxiv.org/pdf/1704.01212.pdf)

ðŸ”µ [**Attention Is All You Need**](https://arxiv.org/pdf/1706.03762.pdf)
  
ðŸ”µ [**Neural Machine Translation by Jointly Learning to Align and Translate**](https://arxiv.org/pdf/1409.0473.pdf)

ðŸ”µ [**Identity Mappings in Deep Residual Networks**](https://arxiv.org/pdf/1603.05027.pdf)

ðŸ”µ [**A Simple Neural Network Module for Relational Reasoning**](https://arxiv.org/pdf/1706.01427.pdf)

+ [**Variational Lossy Autoencoder**](https://arxiv.org/pdf/1611.02731.pdf)

ðŸ”µ [**Relational Recurrent Neural Networks**](https://arxiv.org/pdf/1806.01822.pdf)

ðŸ”µ [**Quantifying the Rise and Fall of Complexity in Closed Systems: The Coffee Automaton**](https://arxiv.org/pdf/1405.6903.pdf)

ðŸ”µ [**Neural Turing Machines**](https://arxiv.org/pdf/1410.5401.pdf)

ðŸ”µ [**Deep Speech 2: End-to-End Speech Recognition in English and Mandarin**](https://arxiv.org/pdf/1512.02595.pdf)

ðŸ”µ [**Scaling Laws for Neural Language Models**](https://arxiv.org/pdf/2001.08361.pdf)

ðŸ”µ [**A Tutorial Introduction to the Minimum Description Length Principle**](https://arxiv.org/pdf/math/0406077.pdf)

ðŸ”µ [**Machine Super Intelligence**](http://www.vetta.org/documents/Machine_Super_Intelligence.pdf)

ðŸ”µ [**Kolmogorov Complexity (from page 434)**](https://www.lirmm.fr/~ashen/kolmbook-eng-scan.pdf)

ðŸ”µ [**CS231n Convolutional Neural Networks for Visual Recognition**](https://cs231n.github.io/)
